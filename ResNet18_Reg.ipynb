{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeSC4zKuSeje",
        "outputId": "ec206211-2bb2-4d55-832d-cf9a4dcbcced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "6RXT1oAZSqW7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update with the correct paths for your dataset\n",
        "dataset_folder = '/content/drive/MyDrive/Dataset'\n",
        "train_csv_path = os.path.join(dataset_folder, '/content/drive/MyDrive/Dataset/primary_train.csv')\n",
        "val_csv_path = os.path.join(dataset_folder, '/content/drive/MyDrive/Dataset/val(unseen).csv')\n",
        "test_csv_path = os.path.join(dataset_folder, '/content/drive/MyDrive/Dataset/test(unseen).csv')\n",
        "image_folder = os.path.join(dataset_folder, 'all_images')"
      ],
      "metadata": {
        "id": "r7Cxj5isSqTz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom dataset class for the dataset with the new structure\n",
        "class CustomDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, image_folder, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_folder = image_folder\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_folder, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = int(self.data.iloc[idx, 1])  # Assuming the label column is at index 1\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "_s8nuPifSqQq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "7MWFEBt9SqNq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Update model initialization for ResNet-18 with Dropout, L2 Regularization, and BatchNorm\n",
        "class ResNet18Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_prob=0.5, l2_reg=0.001):\n",
        "        super(ResNet18Classifier, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "        in_features = self.resnet18.fc.in_features\n",
        "\n",
        "        # Replace the fully connected layer with dropout and L2 regularization\n",
        "        self.resnet18.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, num_classes),\n",
        "            nn.Dropout(p=dropout_prob),\n",
        "            nn.BatchNorm1d(num_classes),  # Batch normalization after dropout\n",
        "        )\n",
        "\n",
        "        # Add L2 regularization to all Conv2d layers\n",
        "        for module in self.resnet18.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                module.weight_regularizer = torch.nn.Parameter(\n",
        "                    l2_reg * torch.ones_like(module.weight)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet18(x)\n",
        "\n",
        "# Instantiate the ResNet18-based model with ensemble learning components\n",
        "model = ResNet18Classifier(num_classes=9, dropout_prob=0.5, l2_reg=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YofrI7mSqKj",
        "outputId": "3464b493-1eb0-413b-91c4-cdcadbe83e92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 247MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets using the new structure\n",
        "train_dataset = CustomDatasetFromCSV(train_csv_path, image_folder, transform=transform)\n",
        "val_dataset = CustomDatasetFromCSV(val_csv_path, image_folder, transform=transform)\n",
        "test_dataset = CustomDatasetFromCSV(test_csv_path, image_folder, transform=transform)"
      ],
      "metadata": {
        "id": "l1_BkTnaSqDK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "# Split the datasets and create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "z37AIyEMSqAV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Update the optimizer to use ResNet-18 parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0)  # L2 regularization as weight_decay\n",
        "\n",
        "# Add learning rate decay\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urN2QhdsSp9C",
        "outputId": "e194cbd2-78b6-4656-af58-a88756348831"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet18Classifier(\n",
              "  (resnet18): ResNet(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=9, bias=True)\n",
              "      (1): Dropout(p=0.5, inplace=False)\n",
              "      (2): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to keep track of the best model\n",
        "best_validation_loss = float('inf')\n",
        "best_model_path = '/content/drive/MyDrive/Dataset/ResNet18_model_3.pth'\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0.0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for images, labels in train_loader:  # Assuming you have defined train_loader\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()  # Zero the gradient buffers\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Loss computation\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute training accuracy and loss\n",
        "    training_accuracy = accuracy_score(true_labels, predictions)\n",
        "    training_loss = total_loss / len(train_loader)  # Compute the average loss\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_total_loss = 0.0\n",
        "    val_predictions = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    for images, labels in val_loader:  # Assuming you have defined val_loader\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "            val_loss = criterion(outputs, labels)\n",
        "            val_total_loss += val_loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_predictions.extend(predicted.cpu().numpy())\n",
        "            val_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute validation accuracy and loss\n",
        "    validation_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
        "    validation_loss = val_total_loss / len(val_loader)  # Compute the average loss\n",
        "\n",
        "    # Save the model with the best validation loss\n",
        "    if validation_loss < best_validation_loss:\n",
        "        best_validation_loss = validation_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print the metrics for this epoch\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
        "    print(f\"Training Loss: {training_loss:.4f}, Training Accuracy: {training_accuracy:.4f}\")\n",
        "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "    print(f\"Learning Rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'resnet18_model_3.pth')\n",
        "\n",
        "# Save the best model based on validation loss\n",
        "if os.path.exists(best_model_path):\n",
        "    os.remove(best_model_path)  # Remove previous best model\n",
        "torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"Best model saved at: {best_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5jYRATsSp5c",
        "outputId": "36e6c628-fa64-4e35-9d62-e79f5b8a4fe1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100:\n",
            "Training Loss: 1.3380, Training Accuracy: 0.5414\n",
            "Validation Loss: 1.6891, Validation Accuracy: 0.4765\n",
            "Learning Rate: [0.001]\n",
            "Epoch 2/100:\n",
            "Training Loss: 1.1888, Training Accuracy: 0.6043\n",
            "Validation Loss: 1.4478, Validation Accuracy: 0.5570\n",
            "Learning Rate: [0.001]\n",
            "Epoch 3/100:\n",
            "Training Loss: 1.0915, Training Accuracy: 0.6264\n",
            "Validation Loss: 1.3684, Validation Accuracy: 0.6309\n",
            "Learning Rate: [0.001]\n",
            "Epoch 4/100:\n",
            "Training Loss: 1.0963, Training Accuracy: 0.6196\n",
            "Validation Loss: 1.6498, Validation Accuracy: 0.4966\n",
            "Learning Rate: [0.001]\n",
            "Epoch 5/100:\n",
            "Training Loss: 1.0930, Training Accuracy: 0.6173\n",
            "Validation Loss: 1.4890, Validation Accuracy: 0.4631\n",
            "Learning Rate: [0.001]\n",
            "Epoch 6/100:\n",
            "Training Loss: 1.0899, Training Accuracy: 0.6128\n",
            "Validation Loss: 1.6540, Validation Accuracy: 0.4497\n",
            "Learning Rate: [0.001]\n",
            "Epoch 7/100:\n",
            "Training Loss: 1.0694, Training Accuracy: 0.6168\n",
            "Validation Loss: 1.5461, Validation Accuracy: 0.5101\n",
            "Learning Rate: [0.001]\n",
            "Epoch 8/100:\n",
            "Training Loss: 1.0753, Training Accuracy: 0.6162\n",
            "Validation Loss: 1.3965, Validation Accuracy: 0.5369\n",
            "Learning Rate: [0.001]\n",
            "Epoch 9/100:\n",
            "Training Loss: 1.0051, Training Accuracy: 0.6264\n",
            "Validation Loss: 1.3241, Validation Accuracy: 0.6174\n",
            "Learning Rate: [0.001]\n",
            "Epoch 10/100:\n",
            "Training Loss: 1.0245, Training Accuracy: 0.6400\n",
            "Validation Loss: 1.4147, Validation Accuracy: 0.5772\n",
            "Learning Rate: [0.001]\n",
            "Epoch 11/100:\n",
            "Training Loss: 1.0209, Training Accuracy: 0.6315\n",
            "Validation Loss: 1.3396, Validation Accuracy: 0.5973\n",
            "Learning Rate: [0.001]\n",
            "Epoch 12/100:\n",
            "Training Loss: 0.9671, Training Accuracy: 0.6474\n",
            "Validation Loss: 1.3104, Validation Accuracy: 0.6309\n",
            "Learning Rate: [0.001]\n",
            "Epoch 13/100:\n",
            "Training Loss: 1.0053, Training Accuracy: 0.6304\n",
            "Validation Loss: 1.2937, Validation Accuracy: 0.5839\n",
            "Learning Rate: [0.001]\n",
            "Epoch 14/100:\n",
            "Training Loss: 0.9405, Training Accuracy: 0.6570\n",
            "Validation Loss: 1.4899, Validation Accuracy: 0.5168\n",
            "Learning Rate: [0.001]\n",
            "Epoch 15/100:\n",
            "Training Loss: 0.9637, Training Accuracy: 0.6287\n",
            "Validation Loss: 1.4090, Validation Accuracy: 0.5034\n",
            "Learning Rate: [0.001]\n",
            "Epoch 16/100:\n",
            "Training Loss: 1.0103, Training Accuracy: 0.6077\n",
            "Validation Loss: 1.3423, Validation Accuracy: 0.5570\n",
            "Learning Rate: [0.001]\n",
            "Epoch 17/100:\n",
            "Training Loss: 0.9737, Training Accuracy: 0.6179\n",
            "Validation Loss: 1.5216, Validation Accuracy: 0.4765\n",
            "Learning Rate: [0.001]\n",
            "Epoch 18/100:\n",
            "Training Loss: 0.9929, Training Accuracy: 0.6179\n",
            "Validation Loss: 1.3382, Validation Accuracy: 0.5973\n",
            "Learning Rate: [0.001]\n",
            "Epoch 19/100:\n",
            "Training Loss: 0.9978, Training Accuracy: 0.6179\n",
            "Validation Loss: 1.4050, Validation Accuracy: 0.5168\n",
            "Learning Rate: [0.001]\n",
            "Epoch 20/100:\n",
            "Training Loss: 0.9394, Training Accuracy: 0.6406\n",
            "Validation Loss: 1.4447, Validation Accuracy: 0.4899\n",
            "Learning Rate: [0.001]\n",
            "Epoch 21/100:\n",
            "Training Loss: 0.9506, Training Accuracy: 0.6434\n",
            "Validation Loss: 1.2971, Validation Accuracy: 0.6242\n",
            "Learning Rate: [0.001]\n",
            "Epoch 22/100:\n",
            "Training Loss: 0.9709, Training Accuracy: 0.6247\n",
            "Validation Loss: 1.4261, Validation Accuracy: 0.5101\n",
            "Learning Rate: [0.001]\n",
            "Epoch 23/100:\n",
            "Training Loss: 0.9557, Training Accuracy: 0.6241\n",
            "Validation Loss: 1.5303, Validation Accuracy: 0.4698\n",
            "Learning Rate: [0.001]\n",
            "Epoch 24/100:\n",
            "Training Loss: 0.9325, Training Accuracy: 0.6310\n",
            "Validation Loss: 1.4247, Validation Accuracy: 0.5570\n",
            "Learning Rate: [0.001]\n",
            "Epoch 25/100:\n",
            "Training Loss: 0.9460, Training Accuracy: 0.6395\n",
            "Validation Loss: 1.4520, Validation Accuracy: 0.5034\n",
            "Learning Rate: [0.001]\n",
            "Epoch 26/100:\n",
            "Training Loss: 0.9638, Training Accuracy: 0.6293\n",
            "Validation Loss: 1.4219, Validation Accuracy: 0.5101\n",
            "Learning Rate: [0.001]\n",
            "Epoch 27/100:\n",
            "Training Loss: 0.9542, Training Accuracy: 0.6298\n",
            "Validation Loss: 1.3499, Validation Accuracy: 0.5973\n",
            "Learning Rate: [0.001]\n",
            "Epoch 28/100:\n",
            "Training Loss: 0.9174, Training Accuracy: 0.6293\n",
            "Validation Loss: 1.2356, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.001]\n",
            "Epoch 29/100:\n",
            "Training Loss: 0.8845, Training Accuracy: 0.6638\n",
            "Validation Loss: 1.3802, Validation Accuracy: 0.5369\n",
            "Learning Rate: [0.001]\n",
            "Epoch 30/100:\n",
            "Training Loss: 0.9742, Training Accuracy: 0.6179\n",
            "Validation Loss: 1.2524, Validation Accuracy: 0.6443\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 31/100:\n",
            "Training Loss: 0.9192, Training Accuracy: 0.6338\n",
            "Validation Loss: 1.2147, Validation Accuracy: 0.6242\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 32/100:\n",
            "Training Loss: 0.9047, Training Accuracy: 0.6366\n",
            "Validation Loss: 1.1832, Validation Accuracy: 0.6443\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 33/100:\n",
            "Training Loss: 0.9222, Training Accuracy: 0.6253\n",
            "Validation Loss: 1.2571, Validation Accuracy: 0.6242\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 34/100:\n",
            "Training Loss: 0.8652, Training Accuracy: 0.6650\n",
            "Validation Loss: 1.2400, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 35/100:\n",
            "Training Loss: 0.8782, Training Accuracy: 0.6633\n",
            "Validation Loss: 1.2611, Validation Accuracy: 0.5973\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 36/100:\n",
            "Training Loss: 0.9214, Training Accuracy: 0.6259\n",
            "Validation Loss: 1.2141, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 37/100:\n",
            "Training Loss: 0.8921, Training Accuracy: 0.6327\n",
            "Validation Loss: 1.2207, Validation Accuracy: 0.6242\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 38/100:\n",
            "Training Loss: 0.9087, Training Accuracy: 0.6276\n",
            "Validation Loss: 1.1645, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 39/100:\n",
            "Training Loss: 0.8643, Training Accuracy: 0.6627\n",
            "Validation Loss: 1.2241, Validation Accuracy: 0.6242\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 40/100:\n",
            "Training Loss: 0.9305, Training Accuracy: 0.6241\n",
            "Validation Loss: 1.2437, Validation Accuracy: 0.5839\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 41/100:\n",
            "Training Loss: 0.8720, Training Accuracy: 0.6616\n",
            "Validation Loss: 1.2355, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 42/100:\n",
            "Training Loss: 0.8969, Training Accuracy: 0.6395\n",
            "Validation Loss: 1.1931, Validation Accuracy: 0.6174\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 43/100:\n",
            "Training Loss: 0.9402, Training Accuracy: 0.6185\n",
            "Validation Loss: 1.1138, Validation Accuracy: 0.6309\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 44/100:\n",
            "Training Loss: 0.8921, Training Accuracy: 0.6434\n",
            "Validation Loss: 1.2622, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 45/100:\n",
            "Training Loss: 0.9334, Training Accuracy: 0.6179\n",
            "Validation Loss: 1.1961, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 46/100:\n",
            "Training Loss: 0.9002, Training Accuracy: 0.6395\n",
            "Validation Loss: 1.2272, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 47/100:\n",
            "Training Loss: 0.9030, Training Accuracy: 0.6463\n",
            "Validation Loss: 1.1504, Validation Accuracy: 0.6309\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 48/100:\n",
            "Training Loss: 0.9217, Training Accuracy: 0.6247\n",
            "Validation Loss: 1.1830, Validation Accuracy: 0.5973\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 49/100:\n",
            "Training Loss: 0.8840, Training Accuracy: 0.6480\n",
            "Validation Loss: 1.3617, Validation Accuracy: 0.5503\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 50/100:\n",
            "Training Loss: 0.8939, Training Accuracy: 0.6349\n",
            "Validation Loss: 1.2357, Validation Accuracy: 0.5772\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 51/100:\n",
            "Training Loss: 0.9276, Training Accuracy: 0.6156\n",
            "Validation Loss: 1.2273, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 52/100:\n",
            "Training Loss: 0.9267, Training Accuracy: 0.6190\n",
            "Validation Loss: 1.1688, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 53/100:\n",
            "Training Loss: 0.8959, Training Accuracy: 0.6310\n",
            "Validation Loss: 1.2912, Validation Accuracy: 0.5839\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 54/100:\n",
            "Training Loss: 0.8904, Training Accuracy: 0.6366\n",
            "Validation Loss: 1.2445, Validation Accuracy: 0.6040\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 55/100:\n",
            "Training Loss: 0.8821, Training Accuracy: 0.6474\n",
            "Validation Loss: 1.2235, Validation Accuracy: 0.6174\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 56/100:\n",
            "Training Loss: 0.8998, Training Accuracy: 0.6253\n",
            "Validation Loss: 1.2800, Validation Accuracy: 0.5973\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 57/100:\n",
            "Training Loss: 0.9048, Training Accuracy: 0.6440\n",
            "Validation Loss: 1.2163, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 58/100:\n",
            "Training Loss: 0.9129, Training Accuracy: 0.6338\n",
            "Validation Loss: 1.2543, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 59/100:\n",
            "Training Loss: 0.8951, Training Accuracy: 0.6338\n",
            "Validation Loss: 1.2552, Validation Accuracy: 0.6107\n",
            "Learning Rate: [0.0001]\n",
            "Epoch 60/100:\n",
            "Training Loss: 0.8570, Training Accuracy: 0.6661\n",
            "Validation Loss: 1.2913, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 61/100:\n",
            "Training Loss: 0.8830, Training Accuracy: 0.6570\n",
            "Validation Loss: 1.3761, Validation Accuracy: 0.5436\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 62/100:\n",
            "Training Loss: 0.9238, Training Accuracy: 0.6406\n",
            "Validation Loss: 1.3327, Validation Accuracy: 0.5772\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 63/100:\n",
            "Training Loss: 0.9140, Training Accuracy: 0.6185\n",
            "Validation Loss: 1.3178, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 64/100:\n",
            "Training Loss: 0.8860, Training Accuracy: 0.6497\n",
            "Validation Loss: 1.2321, Validation Accuracy: 0.5973\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 65/100:\n",
            "Training Loss: 0.8804, Training Accuracy: 0.6310\n",
            "Validation Loss: 1.2813, Validation Accuracy: 0.5973\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 66/100:\n",
            "Training Loss: 0.9020, Training Accuracy: 0.6423\n",
            "Validation Loss: 1.3094, Validation Accuracy: 0.5705\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 67/100:\n",
            "Training Loss: 0.8477, Training Accuracy: 0.6633\n",
            "Validation Loss: 1.2695, Validation Accuracy: 0.5839\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 68/100:\n",
            "Training Loss: 0.8694, Training Accuracy: 0.6400\n",
            "Validation Loss: 1.2095, Validation Accuracy: 0.6309\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 69/100:\n",
            "Training Loss: 0.8836, Training Accuracy: 0.6412\n",
            "Validation Loss: 1.2485, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 70/100:\n",
            "Training Loss: 0.8802, Training Accuracy: 0.6463\n",
            "Validation Loss: 1.2291, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 71/100:\n",
            "Training Loss: 0.9035, Training Accuracy: 0.6304\n",
            "Validation Loss: 1.2341, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 72/100:\n",
            "Training Loss: 0.8745, Training Accuracy: 0.6412\n",
            "Validation Loss: 1.2517, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 73/100:\n",
            "Training Loss: 0.8643, Training Accuracy: 0.6423\n",
            "Validation Loss: 1.2785, Validation Accuracy: 0.5973\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 74/100:\n",
            "Training Loss: 0.8775, Training Accuracy: 0.6531\n",
            "Validation Loss: 1.2107, Validation Accuracy: 0.6174\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 75/100:\n",
            "Training Loss: 0.9134, Training Accuracy: 0.6423\n",
            "Validation Loss: 1.2092, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 76/100:\n",
            "Training Loss: 0.8658, Training Accuracy: 0.6655\n",
            "Validation Loss: 1.2879, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 77/100:\n",
            "Training Loss: 0.9373, Training Accuracy: 0.6139\n",
            "Validation Loss: 1.3337, Validation Accuracy: 0.5772\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 78/100:\n",
            "Training Loss: 0.8775, Training Accuracy: 0.6548\n",
            "Validation Loss: 1.3125, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 79/100:\n",
            "Training Loss: 0.8887, Training Accuracy: 0.6480\n",
            "Validation Loss: 1.2319, Validation Accuracy: 0.6174\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 80/100:\n",
            "Training Loss: 0.8614, Training Accuracy: 0.6514\n",
            "Validation Loss: 1.2058, Validation Accuracy: 0.6040\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 81/100:\n",
            "Training Loss: 0.8886, Training Accuracy: 0.6287\n",
            "Validation Loss: 1.2224, Validation Accuracy: 0.6242\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 82/100:\n",
            "Training Loss: 0.9499, Training Accuracy: 0.6230\n",
            "Validation Loss: 1.2005, Validation Accuracy: 0.6174\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 83/100:\n",
            "Training Loss: 0.9048, Training Accuracy: 0.6315\n",
            "Validation Loss: 1.2148, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 84/100:\n",
            "Training Loss: 0.9127, Training Accuracy: 0.6293\n",
            "Validation Loss: 1.2404, Validation Accuracy: 0.6174\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 85/100:\n",
            "Training Loss: 0.8650, Training Accuracy: 0.6701\n",
            "Validation Loss: 1.2660, Validation Accuracy: 0.6174\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 86/100:\n",
            "Training Loss: 0.8720, Training Accuracy: 0.6576\n",
            "Validation Loss: 1.1945, Validation Accuracy: 0.6242\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 87/100:\n",
            "Training Loss: 0.8974, Training Accuracy: 0.6525\n",
            "Validation Loss: 1.2683, Validation Accuracy: 0.6309\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 88/100:\n",
            "Training Loss: 0.9120, Training Accuracy: 0.6474\n",
            "Validation Loss: 1.2549, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 89/100:\n",
            "Training Loss: 0.8857, Training Accuracy: 0.6406\n",
            "Validation Loss: 1.2374, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1e-05]\n",
            "Epoch 90/100:\n",
            "Training Loss: 0.8947, Training Accuracy: 0.6468\n",
            "Validation Loss: 1.2361, Validation Accuracy: 0.5973\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 91/100:\n",
            "Training Loss: 0.9137, Training Accuracy: 0.6230\n",
            "Validation Loss: 1.3144, Validation Accuracy: 0.5839\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 92/100:\n",
            "Training Loss: 0.8861, Training Accuracy: 0.6446\n",
            "Validation Loss: 1.2570, Validation Accuracy: 0.6174\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 93/100:\n",
            "Training Loss: 0.9143, Training Accuracy: 0.6241\n",
            "Validation Loss: 1.2178, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 94/100:\n",
            "Training Loss: 0.8511, Training Accuracy: 0.6514\n",
            "Validation Loss: 1.2549, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 95/100:\n",
            "Training Loss: 0.9206, Training Accuracy: 0.6253\n",
            "Validation Loss: 1.2345, Validation Accuracy: 0.6040\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 96/100:\n",
            "Training Loss: 0.8967, Training Accuracy: 0.6576\n",
            "Validation Loss: 1.2170, Validation Accuracy: 0.6242\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 97/100:\n",
            "Training Loss: 0.8717, Training Accuracy: 0.6604\n",
            "Validation Loss: 1.2982, Validation Accuracy: 0.5772\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 98/100:\n",
            "Training Loss: 0.8887, Training Accuracy: 0.6497\n",
            "Validation Loss: 1.3058, Validation Accuracy: 0.5906\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 99/100:\n",
            "Training Loss: 0.8717, Training Accuracy: 0.6378\n",
            "Validation Loss: 1.2377, Validation Accuracy: 0.6107\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch 100/100:\n",
            "Training Loss: 0.8886, Training Accuracy: 0.6491\n",
            "Validation Loss: 1.2918, Validation Accuracy: 0.5772\n",
            "Learning Rate: [1.0000000000000002e-06]\n",
            "Best model saved at: /content/drive/MyDrive/Dataset/ResNet18_model_3.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# Load the test data\n",
        "test_csv_file = '/content/drive/MyDrive/Dataset/test(unseen).csv'\n",
        "test_data = pd.read_csv(test_csv_file)\n",
        "\n",
        "# Custom dataset class for test data\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data, image_dir, transform=None):\n",
        "        self.data = data\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.data.iloc[idx, 0])\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Define transforms for test data (similar to training)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create a dataset and data loader for test data\n",
        "test_image_dir = '/content/drive/MyDrive/Dataset/all_images'\n",
        "test_dataset = TestDataset(test_data, test_image_dir, transform=test_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Load the trained model (CustomResNet18)\n",
        "model = ResNet18Classifier(num_classes=9)  # Use the custom model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Dataset/ResNet18_model_3.pth'))\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Testing loop\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUvuk1H9TTzW",
        "outputId": "016a1e36-41d9-44ae-fed2-bb8686058ae4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [5, 2, 8, 0, 2, 2, 3, 1, 0, 0, 7, 2, 0, 7, 0, 2, 3, 3, 0, 8, 3, 0, 7, 3, 4, 8, 1, 7, 3, 7, 7, 3, 2, 3, 8, 0, 5, 3, 3, 8, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Assuming ground truth labels for the test data are in test_data['label']\n",
        "# Replace 'test_data['label']' with the actual ground truth labels from your test data\n",
        "\n",
        "# Calculate accuracy\n",
        "ground_truth_labels = test_data['label'].tolist()  # Ground truth labels\n",
        "accuracy = accuracy_score(ground_truth_labels, predictions)\n",
        "\n",
        "# Calculate class-wise accuracy using a confusion matrix\n",
        "confusion = confusion_matrix(ground_truth_labels, predictions)\n",
        "class_accuracy = confusion.diagonal() / confusion.sum(axis=1)\n",
        "\n",
        "# Print the overall accuracy\n",
        "print(\"Overall Accuracy:\", accuracy * 100, \"%\")\n",
        "\n",
        "# Print class-wise accuracy\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    print(f\"Class {i}: {acc * 100:.2f}%\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "report = classification_report(ground_truth_labels, predictions, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\", \"Class 4\", \"Class 5\", \"Class 6\", \"Class 7\"])\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv0lDm7nTTv3",
        "outputId": "3dcd070e-51d1-4ebc-f1a8-b1b214acaef5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 53.65853658536586 %\n",
            "Class 0: 100.00%\n",
            "Class 1: 28.57%\n",
            "Class 2: 100.00%\n",
            "Class 3: 62.50%\n",
            "Class 4: 50.00%\n",
            "Class 5: 28.57%\n",
            "Class 6: 54.55%\n",
            "Class 7: nan%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.50      1.00      0.67         4\n",
            "     Class 1       1.00      0.29      0.44         7\n",
            "     Class 2       0.33      1.00      0.50         2\n",
            "     Class 3       0.50      0.62      0.56         8\n",
            "     Class 4       0.50      0.50      0.50         2\n",
            "     Class 5       1.00      0.29      0.44         7\n",
            "     Class 6       1.00      0.55      0.71        11\n",
            "     Class 7       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.54        41\n",
            "   macro avg       0.60      0.53      0.48        41\n",
            "weighted avg       0.80      0.54      0.56        41\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-2f1cc2e62bf1>:12: RuntimeWarning: invalid value encountered in divide\n",
            "  class_accuracy = confusion.diagonal() / confusion.sum(axis=1)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aghVC6i_TTqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}